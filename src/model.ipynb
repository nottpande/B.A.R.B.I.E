{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformers import Transformer\n",
    "from Pipeline import TextPreprocessingPipeline\n",
    "from BPE_tokenizer import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataframe is (300000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Kannada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hes a scientist.</td>\n",
       "      <td>ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'But we speak the truth aur ye sach hai ke Guj...</td>\n",
       "      <td>\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8 lakh crore have been looted.</td>\n",
       "      <td>ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I read a lot into this as well.</td>\n",
       "      <td>ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She was found dead with the phone's battery ex...</td>\n",
       "      <td>ಆಕೆಯ ತಲೆಯ ಹತ್ತಿರ ಇರಿಸಿಕೊಂಡಿದ್ದ ಫೋನ್‌ನ ಬ್ಯಾಟರಿ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                                   Hes a scientist.   \n",
       "1  'But we speak the truth aur ye sach hai ke Guj...   \n",
       "2                     8 lakh crore have been looted.   \n",
       "3                    I read a lot into this as well.   \n",
       "4  She was found dead with the phone's battery ex...   \n",
       "\n",
       "                                             Kannada  \n",
       "0                            ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.  \n",
       "1  \"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್...  \n",
       "2                           ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.  \n",
       "3                  ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.  \n",
       "4  ಆಕೆಯ ತಲೆಯ ಹತ್ತಿರ ಇರಿಸಿಕೊಂಡಿದ್ದ ಫೋನ್‌ನ ಬ್ಯಾಟರಿ ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/dataset_kag.csv')\n",
    "print(f'Size of the dataframe is {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Kannada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Congress leader in Lok Sabha, Mallikarjun Kharge.</td>\n",
       "      <td>ಮಲ್ಲಿಕಾರ್ಜುನ ಖರ್ಗೆ, ಲೋಕಸಭೆಯ ಕಾಂಗ್ರೆಸ್‌ ಪಕ್ಷದ ನಾಯಕ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRINAGAR: Ahead of the Republic Day celebratio...</td>\n",
       "      <td>ಶ್ರೀನಗರ: ದೇಶಾದ್ಯಂತ ಸ್ವಾತಂತ್ರ್ಯೋತ್ಸವದ ಸಂಭ್ರಮಕ್ಕ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You will have physical and mental happiness.</td>\n",
       "      <td>ಶಾರೀರಿಕವಾಗಿ ಮತ್ತು ಮಾನಸಿಕವಾಗಿ ತುಂಬಾ ಸಂತೋಷದಿಂದ ಇ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ Pictures on page 29]</td>\n",
       "      <td>[ ಪುಟ 29ರಲ್ಲಿರುವ ಚಿತ್ರ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why did</td>\n",
       "      <td>ಯಾಕೆ ಮಾಡಿಸಿದ್ದು?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  Congress leader in Lok Sabha, Mallikarjun Kharge.   \n",
       "1  SRINAGAR: Ahead of the Republic Day celebratio...   \n",
       "2       You will have physical and mental happiness.   \n",
       "3                             [ Pictures on page 29]   \n",
       "4                                            Why did   \n",
       "\n",
       "                                             Kannada  \n",
       "0  ಮಲ್ಲಿಕಾರ್ಜುನ ಖರ್ಗೆ, ಲೋಕಸಭೆಯ ಕಾಂಗ್ರೆಸ್‌ ಪಕ್ಷದ ನಾಯಕ  \n",
       "1  ಶ್ರೀನಗರ: ದೇಶಾದ್ಯಂತ ಸ್ವಾತಂತ್ರ್ಯೋತ್ಸವದ ಸಂಭ್ರಮಕ್ಕ...  \n",
       "2  ಶಾರೀರಿಕವಾಗಿ ಮತ್ತು ಮಾನಸಿಕವಾಗಿ ತುಂಬಾ ಸಂತೋಷದಿಂದ ಇ...  \n",
       "3                            [ ಪುಟ 29ರಲ್ಲಿರುವ ಚಿತ್ರ]  \n",
       "4                                   ಯಾಕೆ ಮಾಡಿಸಿದ್ದು?  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.sample(20000).reset_index(drop=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PreProcess:\n",
    "    def __init__(self):\n",
    "        print(\"Loading the required files\")\n",
    "        self.english_contractions = '../Data/english_contractions.json'\n",
    "        if not os.path.isfile(self.english_contractions):\n",
    "            raise FileNotFoundError(\"Contraction file does not exist\")\n",
    "        else:\n",
    "            print(\"JSON file exists at location\")\n",
    "\n",
    "        # Initialize normalizers\n",
    "        self.text_processor_eng = PP.TextNormalizerEnglish(self.english_contractions)\n",
    "        self.text_processor_kan = PP.TextNormalizerKannada()\n",
    "\n",
    "        # Initialize vocabularies\n",
    "        self.vocab_eng = set()\n",
    "        self.vocab_kan = set()\n",
    "\n",
    "        # Special tokens\n",
    "        self.special_tokens = {\n",
    "            \"<PAD>\": 0,\n",
    "            \"<SOS>\": 1,\n",
    "            \"<EOS>\": 2,\n",
    "            \"<UNK>\": 3\n",
    "        }\n",
    "\n",
    "        # Load BPE tokenizer\n",
    "        self.tokenizer = self.load_tokenizer('../Models/tokenizer.pkl')\n",
    "\n",
    "    def load_tokenizer(self, tokenizer_file):\n",
    "        print(\"Loading BPE tokenizer...\")\n",
    "        with open(tokenizer_file, 'rb') as f:\n",
    "            merges = pickle.load(f)\n",
    "        print(\"BPE tokenizer loaded successfully.\")\n",
    "        print(type(merges))\n",
    "        tokenizer = BPE(corpus=None, vocab_size=None)\n",
    "        tokenizer.merges = merges\n",
    "        return tokenizer\n",
    "\n",
    "    def preprocess_english(self, sentence):\n",
    "        print(\"Normalizing the English sentence\")\n",
    "        normalized_sentence = self.text_processor_eng.normalize(sentence)\n",
    "        self.build_vocabulary(normalized_sentence, lang='english')\n",
    "        return normalized_sentence\n",
    "\n",
    "    def preprocess_kannada(self, sentence):\n",
    "        print(\"Normalizing the Kannada sentence\")\n",
    "        normalized_sentence = self.text_processor_kan.normalize(sentence)\n",
    "        self.build_vocabulary(normalized_sentence, lang='kannada')\n",
    "        return normalized_sentence\n",
    "\n",
    "    def build_vocabulary(self, sentence, lang):\n",
    "        if lang == 'english':\n",
    "            words = sentence.split()\n",
    "            self.vocab_eng.update(words)\n",
    "        elif lang == 'kannada':\n",
    "            words = sentence.split()\n",
    "            self.vocab_kan.update(words)\n",
    "\n",
    "    def find_max_sequence_length(self, sentences):\n",
    "        max_length = max(len(sentence.split()) for sentence in sentences)\n",
    "        return max_length\n",
    "\n",
    "    def pad_sentences(self, sentence, max_length):\n",
    "            # Tokenize using the BPE tokenizer\n",
    "        print(f'Original Sentence : {sentence}')\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        print(f'Generated Tokens : {tokens}')\n",
    "        print(\"Performing Padding\")\n",
    "            # Add <SOS> at the start and <EOS> at the end\n",
    "        padded_sentence = ['<SOS>'] + tokens + ['<EOS>']\n",
    "            # Calculate how many <PAD> tokens are needed\n",
    "        padding_length = max_length - len(padded_sentence)\n",
    "            # Pad with <PAD> token if necessary (post padding)\n",
    "        if padding_length > 0:\n",
    "                padded_sentence += ['<PAD>'] * padding_length\n",
    "        return padded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2994196487.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    preprocess = TextPreprocessingPipeline('C:\\Users\\Amodini\\Downloads\\Barbie\\B.A.R.B.I.E-main\\Data\\english_contractions.json')\u001b[0m\n\u001b[1;37m                                                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "preprocess = TextPreprocessingPipeline('C:\\Users\\Amodini\\Downloads\\Barbie\\B.A.R.B.I.E-main\\Data\\english_contractions.json')\n",
    "eng_sentences = [\n",
    "    \"I'm going to the store.\",\n",
    "    \"This is an example sentence of 7 tokens.\"\n",
    "]\n",
    "kan_sentences = [\n",
    "    \"ನಾನು ಅಂಗಡಿಗೆ ಹೋಗುತ್ತಿದ್ದೇನೆ.\",\n",
    "    \"ಈ ಒಂದು ಉದಾಹರಣೆ ವಾಕ್ಯವಾಗಿದೆ.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(src_vocab_size, tgt_vocab_size, max_seq_len, embedding_dim=512, num_layers=6, expansion_factor=4, n_heads=8):\n",
    "    model = Transformer(\n",
    "        vocab_size=max(src_vocab_size, tgt_vocab_size),\n",
    "        embedding_dim=embedding_dim,\n",
    "        max_seq_len=max_seq_len,\n",
    "        num_layers=num_layers,\n",
    "        expansion_factor=expansion_factor,\n",
    "        n_heads=n_heads\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, src_sequences, tgt_sequences, num_epochs=10, learning_rate=0.001, checkpoint_dir='checkpoints', batch_size=32):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Wrap the data loader with tqdm for progress tracking\n",
    "        with tqdm(total=src_sequences.size(0), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "            for i in range(0, src_sequences.size(0), batch_size):\n",
    "                src_batch = src_sequences[i:i+batch_size]\n",
    "                tgt_batch = tgt_sequences[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(src_batch, tgt_batch, tgt_mask=None)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), tgt_batch.view(-1))\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Compute accuracy\n",
    "                predicted = outputs.argmax(dim=-1)\n",
    "                mask = tgt_batch != 0  # Assume 0 is the padding index\n",
    "                correct_predictions += (predicted == tgt_batch).masked_select(mask).sum().item()\n",
    "                total_tokens += mask.sum().item()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(src_batch.size(0))\n",
    "                accuracy = correct_predictions / total_tokens if total_tokens > 0 else 0\n",
    "                pbar.set_postfix(loss=epoch_loss / (i + src_batch.size(0)), accuracy=accuracy)\n",
    "        \n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f'Model checkpoint saved at {checkpoint_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, src_sequence, max_tgt_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_sequence = src_sequence.unsqueeze(0)  # Add batch dimension\n",
    "        tgt_sequence = torch.zeros((1, max_tgt_len), dtype=torch.long)  # Initial empty target sequence\n",
    "\n",
    "        for i in range(max_tgt_len):\n",
    "            output = model(src_sequence, tgt_sequence, tgt_mask=None)\n",
    "            prediction = output[:, i, :].argmax(dim=-1)\n",
    "            tgt_sequence[:, i] = prediction\n",
    "\n",
    "        return tgt_sequence.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 345752\n",
      "Vocabulary Size: 133039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_tokens = [word.lower() for text in df['Kannada'] for word in text.split()]\n",
    "\n",
    "unique_tokens = set(all_tokens)\n",
    "\n",
    "src_vocabulary_size = len(unique_tokens)\n",
    "print(\"Vocabulary Size:\", src_vocabulary_size)\n",
    "\n",
    "all_tokens2 = [word.lower() for text in df['English'] for word in text.split()]\n",
    "\n",
    "unique_tokens2 = set(all_tokens2)\n",
    "\n",
    "tgt_vocabulary_size = len(unique_tokens2)\n",
    "print(\"Vocabulary Size:\", tgt_vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 135\n",
      "Maximum sentence length: 238\n"
     ]
    }
   ],
   "source": [
    "sentence_lengths1 = df['Kannada'].dropna().apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate the maximum sentence length\n",
    "src_max_length = sentence_lengths1.max()\n",
    "\n",
    "print(\"Maximum sentence length:\", src_max_length)\n",
    "\n",
    "sentence_lengths2 = df['English'].dropna().apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate the maximum sentence length\n",
    "tgt_max_length = sentence_lengths2.max()\n",
    "\n",
    "print(\"Maximum sentence length:\", tgt_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\Amodini\\.vscode\\extensions\\ms-python.python-2024.20.0-win32-x64\\python_files\\python_server.py\", line 130, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 1, in <module>\n",
       "NameError: name 'preprocess' is not defined\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_sequences, tgt_sequences, src_vocab_size, tgt_vocab_size, max_src_len, max_tgt_len = preprocess(dataset['Kannada Sentences'], dataset['English Sentences'],src_vocabulary_size,tgt_vocabulary_size,src_max_length,tgt_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\Amodini\\.vscode\\extensions\\ms-python.python-2024.20.0-win32-x64\\python_files\\python_server.py\", line 130, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 1, in <module>\n",
       "NameError: name 'src_vocab_size' is not defined\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = initialize_model(src_vocab_size, tgt_vocab_size, max_seq_len=max(max_src_len, max_tgt_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\Amodini\\.vscode\\extensions\\ms-python.python-2024.20.0-win32-x64\\python_files\\python_server.py\", line 130, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 1, in <module>\n",
       "NameError: name 'model' is not defined\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(model, src_sequences, tgt_sequences, num_epochs=10, learning_rate=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
