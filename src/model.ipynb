{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformers import Transformer\n",
    "import Preprocessing as PP\n",
    "from BPE_tokenizer import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataframe is (300000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Kannada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hes a scientist.</td>\n",
       "      <td>ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'But we speak the truth aur ye sach hai ke Guj...</td>\n",
       "      <td>\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8 lakh crore have been looted.</td>\n",
       "      <td>ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I read a lot into this as well.</td>\n",
       "      <td>ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She was found dead with the phone's battery ex...</td>\n",
       "      <td>ಆಕೆಯ ತಲೆಯ ಹತ್ತಿರ ಇರಿಸಿಕೊಂಡಿದ್ದ ಫೋನ್‌ನ ಬ್ಯಾಟರಿ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                                   Hes a scientist.   \n",
       "1  'But we speak the truth aur ye sach hai ke Guj...   \n",
       "2                     8 lakh crore have been looted.   \n",
       "3                    I read a lot into this as well.   \n",
       "4  She was found dead with the phone's battery ex...   \n",
       "\n",
       "                                             Kannada  \n",
       "0                            ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.  \n",
       "1  \"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್...  \n",
       "2                           ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.  \n",
       "3                  ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.  \n",
       "4  ಆಕೆಯ ತಲೆಯ ಹತ್ತಿರ ಇರಿಸಿಕೊಂಡಿದ್ದ ಫೋನ್‌ನ ಬ್ಯಾಟರಿ ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/dataset_kag.csv')\n",
    "print(f'Size of the dataframe is {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Kannada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eat clean and simple.</td>\n",
       "      <td>ಸರಳ ಮತ್ತು ಶುದ್ಧವಾದ ಆಹಾರ ನೀಡುತ್ತೆವೆ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This election, however, is different.</td>\n",
       "      <td>ಆದರೆ, ಈ ಚುನಾವಣೆ ವಿಶೇಷವಂತೂ ಹೌದು.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sir (Subhash Sarkar) called me several times a...</td>\n",
       "      <td>ಗುರುಗಳು(ಸುಭಾಷ್ ಸರ್ಕಾರ್)ನನಗೆ ಹಲವು ಬಾರಿ ಕರೆ ಮಾಡಿ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They help fight bacterial infections.</td>\n",
       "      <td>ಇವು ದೇಹ ಬ್ಯಾಕ್ಟೀರಿಯಾ ವಿರುದ್ದ ಹೋರಾಡಲು ನೆರವಾಗುತ್...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Rs 100</td>\n",
       "      <td>ಗಳ ನೋಟು ಸಿಗದೆ 100 ರೂ.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                              Eat clean and simple.   \n",
       "1              This election, however, is different.   \n",
       "2  Sir (Subhash Sarkar) called me several times a...   \n",
       "3              They help fight bacterial infections.   \n",
       "4                                          No Rs 100   \n",
       "\n",
       "                                             Kannada  \n",
       "0                ಸರಳ ಮತ್ತು ಶುದ್ಧವಾದ ಆಹಾರ ನೀಡುತ್ತೆವೆ.  \n",
       "1                    ಆದರೆ, ಈ ಚುನಾವಣೆ ವಿಶೇಷವಂತೂ ಹೌದು.  \n",
       "2  ಗುರುಗಳು(ಸುಭಾಷ್ ಸರ್ಕಾರ್)ನನಗೆ ಹಲವು ಬಾರಿ ಕರೆ ಮಾಡಿ...  \n",
       "3  ಇವು ದೇಹ ಬ್ಯಾಕ್ಟೀರಿಯಾ ವಿರುದ್ದ ಹೋರಾಡಲು ನೆರವಾಗುತ್...  \n",
       "4                              ಗಳ ನೋಟು ಸಿಗದೆ 100 ರೂ.  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.sample(20000).reset_index(drop=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess:\n",
    "    def __init__(self):\n",
    "        print(\"Loading the required files\")\n",
    "        self.english_contractions = '../Data/english_contractions.json'\n",
    "        if not os.path.isfile(self.english_contractions):\n",
    "            raise FileNotFoundError(\"Contraction file does not exist\")\n",
    "        else:\n",
    "            print(\"JSON file exists at location\")\n",
    "\n",
    "        # Initialize normalizers\n",
    "        self.text_processor_eng = PP.TextNormalizerEnglish(self.english_contractions)\n",
    "        self.text_processor_kan = PP.TextNormalizerKannada()\n",
    "\n",
    "        # Initialize vocabularies\n",
    "        self.vocab_eng = set()\n",
    "        self.vocab_kan = set()\n",
    "\n",
    "        # Special tokens\n",
    "        self.special_tokens = {\n",
    "            \"<PAD>\": 0,\n",
    "            \"<SOS>\": 1,\n",
    "            \"<EOS>\": 2,\n",
    "            \"<UNK>\": 3\n",
    "        }\n",
    "\n",
    "        # Load BPE tokenizer\n",
    "        self.tokenizer = self.load_tokenizer('../Models/tokenizer.pkl')\n",
    "\n",
    "    def load_tokenizer(self, tokenizer_file):\n",
    "        print(\"Loading BPE tokenizer...\")\n",
    "        with open(tokenizer_file, 'rb') as f:\n",
    "            merges = pickle.load(f)\n",
    "        print(\"BPE tokenizer loaded successfully.\")\n",
    "        print(type(merges))\n",
    "        tokenizer = BPE(corpus=None, vocab_size=None)\n",
    "        tokenizer.merges = merges\n",
    "        return tokenizer\n",
    "\n",
    "    def preprocess_english(self, sentence):\n",
    "        print(\"Normalizing the English sentence\")\n",
    "        normalized_sentence = self.text_processor_eng.normalize(sentence)\n",
    "        self.build_vocabulary(normalized_sentence, lang='english')\n",
    "        return normalized_sentence\n",
    "\n",
    "    def preprocess_kannada(self, sentence):\n",
    "        print(\"Normalizing the Kannada sentence\")\n",
    "        normalized_sentence = self.text_processor_kan.normalize(sentence)\n",
    "        self.build_vocabulary(normalized_sentence, lang='kannada')\n",
    "        return normalized_sentence\n",
    "\n",
    "    def build_vocabulary(self, sentence, lang):\n",
    "        if lang == 'english':\n",
    "            words = sentence.split()\n",
    "            self.vocab_eng.update(words)\n",
    "        elif lang == 'kannada':\n",
    "            words = sentence.split()\n",
    "            self.vocab_kan.update(words)\n",
    "\n",
    "    def find_max_sequence_length(self, sentences):\n",
    "        max_length = max(len(sentence.split()) for sentence in sentences)\n",
    "        return max_length\n",
    "\n",
    "    def pad_sentences(self, sentence, max_length):\n",
    "            # Tokenize using the BPE tokenizer\n",
    "        print(f'Original Sentence : {sentence}')\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        print(f'Generated Tokens : {tokens}')\n",
    "        print(\"Performing Padding\")\n",
    "            # Add <SOS> at the start and <EOS> at the end\n",
    "        padded_sentence = ['<SOS>'] + tokens + ['<EOS>']\n",
    "            # Calculate how many <PAD> tokens are needed\n",
    "        padding_length = max_length - len(padded_sentence)\n",
    "            # Pad with <PAD> token if necessary (post padding)\n",
    "        if padding_length > 0:\n",
    "                padded_sentence += ['<PAD>'] * padding_length\n",
    "        return padded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the required files\n",
      "JSON file exists at location\n",
      "In Preprocessing.py\n",
      "../Data/english_contractions.json\n",
      "Contractions loaded successfully.\n",
      "Loading BPE tokenizer...\n",
      "BPE tokenizer loaded successfully.\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "preprocess = PreProcess()\n",
    "eng_sentences = [\n",
    "    \"I'm going to the store.\",\n",
    "    \"This is an example sentence of 7 tokens.\"\n",
    "]\n",
    "kan_sentences = [\n",
    "    \"ನಾನು ಅಂಗಡಿಗೆ ಹೋಗುತ್ತಿದ್ದೇನೆ.\",\n",
    "    \"ಈ ಒಂದು ಉದಾಹರಣೆ ವಾಕ್ಯವಾಗಿದೆ.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(src_vocab_size, tgt_vocab_size, max_seq_len, embedding_dim=512, num_layers=6, expansion_factor=4, n_heads=8):\n",
    "    model = Transformer(\n",
    "        vocab_size=max(src_vocab_size, tgt_vocab_size),\n",
    "        embedding_dim=embedding_dim,\n",
    "        max_seq_len=max_seq_len,\n",
    "        num_layers=num_layers,\n",
    "        expansion_factor=expansion_factor,\n",
    "        n_heads=n_heads\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, src_sequences, tgt_sequences, num_epochs=10, learning_rate=0.001, checkpoint_dir='checkpoints', batch_size=32):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Wrap the data loader with tqdm for progress tracking\n",
    "        with tqdm(total=src_sequences.size(0), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "            for i in range(0, src_sequences.size(0), batch_size):\n",
    "                src_batch = src_sequences[i:i+batch_size]\n",
    "                tgt_batch = tgt_sequences[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(src_batch, tgt_batch, tgt_mask=None)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), tgt_batch.view(-1))\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Compute accuracy\n",
    "                predicted = outputs.argmax(dim=-1)\n",
    "                mask = tgt_batch != 0  # Assume 0 is the padding index\n",
    "                correct_predictions += (predicted == tgt_batch).masked_select(mask).sum().item()\n",
    "                total_tokens += mask.sum().item()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(src_batch.size(0))\n",
    "                accuracy = correct_predictions / total_tokens if total_tokens > 0 else 0\n",
    "                pbar.set_postfix(loss=epoch_loss / (i + src_batch.size(0)), accuracy=accuracy)\n",
    "        \n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f'Model checkpoint saved at {checkpoint_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, src_sequence, max_tgt_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_sequence = src_sequence.unsqueeze(0)  # Add batch dimension\n",
    "        tgt_sequence = torch.zeros((1, max_tgt_len), dtype=torch.long)  # Initial empty target sequence\n",
    "\n",
    "        for i in range(max_tgt_len):\n",
    "            output = model(src_sequence, tgt_sequence, tgt_mask=None)\n",
    "            prediction = output[:, i, :].argmax(dim=-1)\n",
    "            tgt_sequence[:, i] = prediction\n",
    "\n",
    "        return tgt_sequence.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sequences, tgt_sequences, src_vocab_size, tgt_vocab_size, max_src_len, max_tgt_len = preprocess_texts(dataset['Kannada Sentences'], dataset['English Sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(src_vocab_size, tgt_vocab_size, max_seq_len=max(max_src_len, max_tgt_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, src_sequences, tgt_sequences, num_epochs=10, learning_rate=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
