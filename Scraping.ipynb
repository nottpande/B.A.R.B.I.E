{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pa8vsv9Gl6Fg"
      },
      "outputs": [],
      "source": [
        "#pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape text from a Wikipedia page\n",
        "def scrape_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the content from <div id=\"mw-content-text\">\n",
        "    content_div = soup.find('div', id='mw-content-text')\n",
        "\n",
        "    if content_div:\n",
        "        # Extract all paragraphs\n",
        "        paragraphs = content_div.find_all('p')\n",
        "        # Combine the text of all paragraphs, ensuring each paragraph is on a new line\n",
        "        article_text = '\\n'.join([para.get_text(strip=True) for para in paragraphs if para.get_text(strip=True)])\n",
        "        return article_text\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# List of Kannada and English article URLs (Add more pairs as needed)\n",
        "article_pairs = [\n",
        "    {\n",
        "        'kannada_url': 'https://kn.wikipedia.org/wiki/%E0%B2%95%E0%B3%81%E0%B2%B5%E0%B3%86%E0%B2%82%E0%B2%AA%E0%B3%81',\n",
        "        'english_url': 'https://en.wikipedia.org/wiki/Kuvempu'\n",
        "    },\n",
        "    {\n",
        "        'kannada_url': 'https://kn.wikipedia.org/wiki/%E0%B2%B8%E0%B3%8D%E0%B2%B5%E0%B2%BE%E0%B2%AE%E0%B2%BF_%E0%B2%B5%E0%B2%BF%E0%B2%B5%E0%B3%87%E0%B2%95%E0%B2%BE%E0%B2%A8%E0%B2%82%E0%B2%A6',\n",
        "        'english_url': 'https://en.wikipedia.org/wiki/Swami_Vivekananda'\n",
        "    },\n",
        "    {\n",
        "        'kannada_url': 'https://kn.wikipedia.org/wiki/%E0%B2%A4%E0%B2%BE%E0%B2%9C%E0%B3%8D_%E0%B2%AE%E0%B2%B9%E0%B2%B2%E0%B3%8D',\n",
        "        'english_url': 'https://en.wikipedia.org/wiki/Taj_Mahal'\n",
        "    },\n",
        "    {\n",
        "        'kannada_url': 'https://kn.wikipedia.org/wiki/%E0%B2%B2%E0%B2%BF%E0%B2%AF%E0%B3%8A%E0%B2%A8%E0%B2%BE%E0%B2%B0%E0%B3%8D%E0%B2%A1%E0%B3%8A_%E0%B2%A1%E0%B2%BF%E0%B2%95%E0%B2%BE%E0%B2%AA%E0%B3%8D%E0%B2%B0%E0%B2%BF%E0%B2%AF%E0%B3%8A',\n",
        "        'english_url': 'https://en.wikipedia.org/wiki/Leonardo_DiCaprio'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create a parallel corpus and save it in CSV format\n",
        "with open('kannada_english_parallel_corpus.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Kannada', 'English'])  # Header\n",
        "\n",
        "    # Loop through each pair of Kannada and English URLs\n",
        "    for pair in article_pairs:\n",
        "        # Scrape Kannada and English text\n",
        "        kannada_text = scrape_text(pair['kannada_url'])\n",
        "        english_text = scrape_text(pair['english_url'])\n",
        "\n",
        "        # Ensure both texts have content before writing to the file\n",
        "        if kannada_text and english_text:\n",
        "            writer.writerow([kannada_text, english_text])\n",
        "\n",
        "print('Scraping and saving done.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_d0jCqHl9Lo",
        "outputId": "f51005c6-92ad-4e21-d861-a1dd1a615f79"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping and saving done.\n"
          ]
        }
      ]
    }
  ]
}